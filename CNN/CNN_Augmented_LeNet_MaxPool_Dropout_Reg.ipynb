{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.image import imread\n",
    "import numpy as np\n",
    "import os\n",
    "from skimage import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "width = 128\n",
    "height = 128\n",
    "\n",
    "X_train_AR   = np.ndarray(shape=(123, height*width), dtype=np.float64)\n",
    "y_train_AR = []\n",
    "X_test_AR   =  np.ndarray(shape=(123, height*width), dtype=np.float64)\n",
    "y_test_AR = []\n",
    "\n",
    "X_test_sketch_AR = np.ndarray(shape=(123, height*width), dtype=np.float64)\n",
    "\n",
    "X_test_Flip   = np.ndarray(shape=(123, height*width), dtype=np.float64)\n",
    "\n",
    "X_test_Rotate   = np.ndarray(shape=(123, height*width), dtype=np.float64)\n",
    "\n",
    "X_test_Zoom_In_1   = np.ndarray(shape=(123, height*width), dtype=np.float64)\n",
    "\n",
    "X_test_Zoom_In_2   = np.ndarray(shape=(123, height*width), dtype=np.float64)\n",
    "\n",
    "\n",
    "\n",
    "X_train_Flip   = np.ndarray(shape=(123, height*width), dtype=np.float64)\n",
    "y_train_Flip = []\n",
    "\n",
    "X_train_Rotate   = np.ndarray(shape=(123, height*width), dtype=np.float64)\n",
    "y_train_Rotate = []\n",
    "\n",
    "X_train_Zoom_In_1   = np.ndarray(shape=(123, height*width), dtype=np.float64)\n",
    "y_train_Zoom_1 = []\n",
    "\n",
    "X_train_Zoom_In_2   = np.ndarray(shape=(123, height*width), dtype=np.float64)\n",
    "y_train_Zoom_2 = []\n",
    "\n",
    "#CUHK\n",
    "\n",
    "X_test_CUHK   =  np.ndarray(shape=(188, height*width), dtype=np.float64)\n",
    "y_test_CUHK = []\n",
    "\n",
    "X_test_sketch_CUHK = np.ndarray(shape=(188, height*width), dtype=np.float64)\n",
    "\n",
    "X_test_Flip_CUHK   = np.ndarray(shape=(188, height*width), dtype=np.float64)\n",
    "\n",
    "X_test_Rotate_CUHK   = np.ndarray(shape=(188, height*width), dtype=np.float64)\n",
    "\n",
    "X_test_Zoom_In_1_CUHK   = np.ndarray(shape=(188, height*width), dtype=np.float64)\n",
    "\n",
    "X_test_Zoom_In_2_CUHK   = np.ndarray(shape=(188, height*width), dtype=np.float64)\n",
    "\n",
    "\n",
    "X_train_CUHK   = np.ndarray(shape=(188, height*width), dtype=np.float64)\n",
    "y_train_CUHK = []\n",
    "X_test_CUHK   =  np.ndarray(shape=(188, height*width), dtype=np.float64)\n",
    "y_test_CUHK = []\n",
    "\n",
    "X_train_Flip_CUHK   = np.ndarray(shape=(188, height*width), dtype=np.float64)\n",
    "y_train_Flip_CUHK = []\n",
    "\n",
    "X_train_Rotate_CUHK   = np.ndarray(shape=(188, height*width), dtype=np.float64)\n",
    "y_train_Rotate_CUHK = []\n",
    "\n",
    "X_train_Zoom_In_1_CUHK   = np.ndarray(shape=(188, height*width), dtype=np.float64)\n",
    "y_train_Zoom_1_CUHK = []\n",
    "\n",
    "X_train_Zoom_In_2_CUHK   = np.ndarray(shape=(188, height*width), dtype=np.float64)\n",
    "y_train_Zoom_2_CUHK = []\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### TEST\n",
    "\n",
    "#directory = '/Users/boyaronur/Desktop/FACE/resized_photos_AR_128/' \n",
    "directory = '/Users/boyaronur/Desktop/FACE/Photo_Gray_AR/' \n",
    "directory_dir  = os.listdir(directory)\n",
    "directory_dir = sorted(directory_dir)\n",
    "os.chdir(directory)\n",
    "for i in range(len(directory_dir)):\n",
    "    img = io.imread(directory_dir[i], as_gray=True)\n",
    "    X_test_AR[i,:] = np.array(img, dtype='float64').flatten()\n",
    "    y_test_AR.append(directory_dir)\n",
    "#### TRAIN\n",
    "\n",
    "directory = '/Users/boyaronur/Desktop/FACE/resized_sketch_AR_128/' \n",
    "directory_dir  = os.listdir(directory)\n",
    "directory_dir = sorted(directory_dir)\n",
    "os.chdir(directory)\n",
    "for j in range(len(directory_dir)):\n",
    "    img = io.imread(directory_dir[j], as_gray=True)\n",
    "    X_train_AR[j,:] = np.array(img, dtype='float64').flatten()\n",
    "    y_train_AR.append(directory_dir[j])\n",
    "    \n",
    "directory = '/Users/boyaronur/Desktop/FACE/AR_Sketch_Aug_Flip/' \n",
    "directory_dir  = os.listdir(directory)\n",
    "directory_dir = sorted(directory_dir)\n",
    "os.chdir(directory)\n",
    "for j in range(len(directory_dir)):\n",
    "    img = io.imread(directory_dir[j], as_gray=True)\n",
    "    X_train_Flip[j,:] = np.array(img, dtype='float64').flatten()\n",
    "    y_train_Flip.append(directory_dir[j])\n",
    "    \n",
    "directory = '/Users/boyaronur/Desktop/FACE/AR_Sketch_Aug_Zoom_In_1.5/' \n",
    "directory_dir  = os.listdir(directory)\n",
    "directory_dir = sorted(directory_dir)\n",
    "os.chdir(directory)\n",
    "for j in range(len(directory_dir)):\n",
    "    img = io.imread(directory_dir[j], as_gray=True)\n",
    "    X_train_Zoom_In_1[j,:] = np.array(img, dtype='float64').flatten()\n",
    "    y_train_Zoom_1.append(directory_dir[j])\n",
    "    \n",
    "directory = '/Users/boyaronur/Desktop/FACE/AR_Sketch_Aug_Zoom_In_2.5/' \n",
    "directory_dir  = os.listdir(directory)\n",
    "directory_dir = sorted(directory_dir)\n",
    "os.chdir(directory)\n",
    "for j in range(len(directory_dir)):\n",
    "    img = io.imread(directory_dir[j], as_gray=True)\n",
    "    X_train_Zoom_In_2[j,:] = np.array(img, dtype='float64').flatten()\n",
    "    y_train_Zoom_2.append(directory_dir[j])\n",
    "    \n",
    "directory = '/Users/boyaronur/Desktop/FACE/AR_Sketch_Aug_Rotate_90/' \n",
    "directory_dir  = os.listdir(directory)\n",
    "directory_dir = sorted(directory_dir)\n",
    "os.chdir(directory)\n",
    "for j in range(len(directory_dir)):\n",
    "    img = io.imread(directory_dir[j], as_gray=True)\n",
    "    X_train_Rotate[j,:] = np.array(img, dtype='float64').flatten()\n",
    "    y_train_Rotate.append(directory_dir[j])\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### AUGMENTED TEST DATA - AR   \n",
    "\n",
    "directory = '/Users/boyaronur/Desktop/FACE/resized_photos_AR_128/' \n",
    "directory_dir  = os.listdir(directory)\n",
    "directory_dir = sorted(directory_dir)\n",
    "os.chdir(directory)\n",
    "for j in range(len(directory_dir)):\n",
    "    img = io.imread(directory_dir[j], as_gray=True)\n",
    "    X_test_sketch_AR[j,:] = np.array(img, dtype='float64').flatten()\n",
    "    #y_train_Rotate.append(directory_dir[j])\n",
    "\n",
    "\n",
    "directory = '/Users/boyaronur/Desktop/FACE/AR_Photo_Aug_Rotate_90/' \n",
    "directory_dir  = os.listdir(directory)\n",
    "directory_dir = sorted(directory_dir)\n",
    "os.chdir(directory)\n",
    "for j in range(len(directory_dir)):\n",
    "    img = io.imread(directory_dir[j], as_gray=True)\n",
    "    X_test_Rotate[j,:] = np.array(img, dtype='float64').flatten()\n",
    "    #y_train_Rotate.append(directory_dir[j])\n",
    "\n",
    "directory = '/Users/boyaronur/Desktop/FACE/AR_Photo_Aug_Flip/' \n",
    "directory_dir  = os.listdir(directory)\n",
    "directory_dir = sorted(directory_dir)\n",
    "os.chdir(directory)\n",
    "for j in range(len(directory_dir)):\n",
    "    img = io.imread(directory_dir[j], as_gray=True)\n",
    "    X_train_Flip[j,:] = np.array(img, dtype='float64').flatten()\n",
    "    #y_train_Rotate.append(directory_dir[j])\n",
    "\n",
    "directory = '/Users/boyaronur/Desktop/FACE/AR_Photo_Aug_Zoom_In_1.5/' \n",
    "directory_dir  = os.listdir(directory)\n",
    "directory_dir = sorted(directory_dir)\n",
    "os.chdir(directory)\n",
    "for j in range(len(directory_dir)):\n",
    "    img = io.imread(directory_dir[j], as_gray=True)\n",
    "    X_test_Zoom_In_1[j,:] = np.array(img, dtype='float64').flatten()\n",
    "    #y_train_Rotate.append(directory_dir[j])\n",
    "    \n",
    "\n",
    "directory = '/Users/boyaronur/Desktop/FACE/AR_Photo_Aug_Zoom_In_2.5/' \n",
    "directory_dir  = os.listdir(directory)\n",
    "directory_dir = sorted(directory_dir)\n",
    "os.chdir(directory)\n",
    "for j in range(len(directory_dir)):\n",
    "    img = io.imread(directory_dir[j], as_gray=True)\n",
    "    X_test_Zoom_In_2[j,:] = np.array(img, dtype='float64').flatten()\n",
    "    #y_train_Rotate.append(directory_dir[j])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### CUHK #####\n",
    "    \n",
    "#### TEST\n",
    "\n",
    "#directory = '/Users/boyaronur/Desktop/FACE/resized_photos_CUHK_128/' \n",
    "directory = '/Users/boyaronur/Desktop/FACE/Photo_Gray_CUHK/' \n",
    "directory_dir  = os.listdir(directory)\n",
    "directory_dir = sorted(directory_dir)\n",
    "os.chdir(directory)\n",
    "for i in range(len(directory_dir)):\n",
    "    img = io.imread(directory_dir[i], as_gray=True)\n",
    "    X_test_CUHK[i,:] = np.array(img, dtype='float64').flatten()\n",
    "    y_test_CUHK.append(directory_dir)\n",
    "#### TRAIN\n",
    "\n",
    "directory = '/Users/boyaronur/Desktop/FACE/resized_sketch_CUHK_128/' \n",
    "directory_dir  = os.listdir(directory)\n",
    "directory_dir = sorted(directory_dir)\n",
    "os.chdir(directory)\n",
    "for j in range(len(directory_dir)):\n",
    "    img = io.imread(directory_dir[j], as_gray=True)\n",
    "    X_train_CUHK[j,:] = np.array(img, dtype='float64').flatten()\n",
    "    y_train_CUHK.append(directory_dir[j])\n",
    "    \n",
    "directory = '/Users/boyaronur/Desktop/FACE/CUHK_Sketch_Aug_Flip/' \n",
    "directory_dir  = os.listdir(directory)\n",
    "directory_dir = sorted(directory_dir)\n",
    "os.chdir(directory)\n",
    "for j in range(len(directory_dir)):\n",
    "    img = io.imread(directory_dir[j], as_gray=True)\n",
    "    X_train_Flip_CUHK[j,:] = np.array(img, dtype='float64').flatten()\n",
    "    y_train_Flip_CUHK.append(directory_dir[j])\n",
    "    \n",
    "directory = '/Users/boyaronur/Desktop/FACE/CUHK_Sketch_Aug_Zoom_In_1.5/' \n",
    "directory_dir  = os.listdir(directory)\n",
    "directory_dir = sorted(directory_dir)\n",
    "os.chdir(directory)\n",
    "for j in range(len(directory_dir)):\n",
    "    img = io.imread(directory_dir[j], as_gray=True)\n",
    "    X_train_Zoom_In_1_CUHK[j,:] = np.array(img, dtype='float64').flatten()\n",
    "    y_train_Zoom_1_CUHK.append(directory_dir[j])\n",
    "    \n",
    "directory = '/Users/boyaronur/Desktop/FACE/CUHK_Sketch_Aug_Zoom_In_2.5/' \n",
    "directory_dir  = os.listdir(directory)\n",
    "directory_dir = sorted(directory_dir)\n",
    "os.chdir(directory)\n",
    "for j in range(len(directory_dir)):\n",
    "    img = io.imread(directory_dir[j], as_gray=True)\n",
    "    X_train_Zoom_In_2_CUHK[j,:] = np.array(img, dtype='float64').flatten()\n",
    "    y_train_Zoom_2_CUHK.append(directory_dir[j])\n",
    "    \n",
    "directory = '/Users/boyaronur/Desktop/FACE/CUHK_Sketch_Aug_Rotate_90/' \n",
    "directory_dir  = os.listdir(directory)\n",
    "directory_dir = sorted(directory_dir)\n",
    "os.chdir(directory)\n",
    "for j in range(len(directory_dir)):\n",
    "    img = io.imread(directory_dir[j], as_gray=True)\n",
    "    X_train_Rotate_CUHK[j,:] = np.array(img, dtype='float64').flatten()\n",
    "    y_train_Rotate_CUHK.append(directory_dir[j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## AUGMENTED TEST DATA - CUHK\n",
    "\n",
    "directory = '/Users/boyaronur/Desktop/FACE/resized_photos_CUHK_128/' \n",
    "directory_dir  = os.listdir(directory)\n",
    "directory_dir = sorted(directory_dir)\n",
    "os.chdir(directory)\n",
    "for j in range(len(directory_dir)):\n",
    "    img = io.imread(directory_dir[j], as_gray=True)\n",
    "    X_test_sketch_CUHK[j,:] = np.array(img, dtype='float64').flatten()\n",
    "    #y_train_Rotate.append(directory_dir[j])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "directory = '/Users/boyaronur/Desktop/FACE/CUHK_Photo_Aug_Rotate_90/' \n",
    "directory_dir  = os.listdir(directory)\n",
    "directory_dir = sorted(directory_dir)\n",
    "os.chdir(directory)\n",
    "for j in range(len(directory_dir)):\n",
    "    img = io.imread(directory_dir[j], as_gray=True)\n",
    "    X_test_Rotate_CUHK[j,:] = np.array(img, dtype='float64').flatten()\n",
    "    #y_train_Rotate.append(directory_dir[j])\n",
    "\n",
    "directory = '/Users/boyaronur/Desktop/FACE/CUHK_Photo_Aug_Flip/' \n",
    "directory_dir  = os.listdir(directory)\n",
    "directory_dir = sorted(directory_dir)\n",
    "os.chdir(directory)\n",
    "for j in range(len(directory_dir)):\n",
    "    img = io.imread(directory_dir[j], as_gray=True)\n",
    "    X_train_Flip_CUHK[j,:] = np.array(img, dtype='float64').flatten()\n",
    "    #y_train_Rotate.append(directory_dir[j])\n",
    "\n",
    "directory = '/Users/boyaronur/Desktop/FACE/CUHK_Photo_Aug_Zoom_In_1.5/' \n",
    "directory_dir  = os.listdir(directory)\n",
    "directory_dir = sorted(directory_dir)\n",
    "os.chdir(directory)\n",
    "for j in range(len(directory_dir)):\n",
    "    img = io.imread(directory_dir[j], as_gray=True)\n",
    "    X_test_Zoom_In_1_CUHK[j,:] = np.array(img, dtype='float64').flatten()\n",
    "    #y_train_Rotate.append(directory_dir[j])\n",
    "    \n",
    "\n",
    "directory = '/Users/boyaronur/Desktop/FACE/CUHK_Photo_Aug_Zoom_In_2.5/' \n",
    "directory_dir  = os.listdir(directory)\n",
    "directory_dir = sorted(directory_dir)\n",
    "os.chdir(directory)\n",
    "for j in range(len(directory_dir)):\n",
    "    img = io.imread(directory_dir[j], as_gray=True)\n",
    "    X_test_Zoom_In_2_CUHK[j,:] = np.array(img, dtype='float64').flatten()\n",
    "    #y_train_Rotate.append(directory_dir[j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(188, 16384)\n",
      "(188, 16384)\n",
      "(188, 16384)\n",
      "(188, 16384)\n",
      "(188, 16384)\n",
      "(188, 16384)\n",
      "(188, 16384)\n",
      "(188, 16384)\n",
      "(188, 16384)\n",
      "(188, 16384)\n"
     ]
    }
   ],
   "source": [
    "print(X_test_CUHK.shape)\n",
    "print(X_train_CUHK.shape)\n",
    "print(X_train_Flip_CUHK.shape)\n",
    "print(X_train_Rotate_CUHK.shape)\n",
    "print(X_train_Zoom_In_1_CUHK.shape)\n",
    "print(X_train_Zoom_In_2_CUHK.shape)\n",
    "\n",
    "print(X_test_Flip_CUHK.shape)\n",
    "print(X_test_Rotate_CUHK.shape)\n",
    "print(X_test_Zoom_In_1_CUHK.shape)\n",
    "print(X_test_Zoom_In_2_CUHK.shape)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tensor   = np.ndarray(shape=(123*5 + 188*5  + 70*6 + 100*6, height*width), dtype=np.float64)\n",
    "#X_tensor_test = np.ndarray(shape=(685, height*width), dtype=np.float64)\n",
    "X_tensor = X_tensor.reshape(2405,128,128,1)\n",
    "#X_tensor_test = X_tensor_test.reshape(685,128,128,1)\n",
    "\n",
    "#X_test_AR = X_test_AR.reshape(123,128,128,1)\n",
    "\n",
    "X_tensor[0:123] = X_train_AR.reshape(123,128,128,1)\n",
    "X_tensor[123:246] = X_train_Flip.reshape(123,128,128,1)\n",
    "X_tensor[246:369] = X_train_Rotate.reshape(123,128,128,1)\n",
    "X_tensor[369:492] = X_train_Zoom_In_1.reshape(123,128,128,1)\n",
    "X_tensor[492:615] = X_train_Zoom_In_2.reshape(123,128,128,1)\n",
    "X_tensor[615:685] = X_test_AR[0:70].reshape(70,128,128,1)\n",
    "X_tensor[685:755] = X_test_Flip[0:70].reshape(70,128,128,1)\n",
    "X_tensor[755:825] = X_test_Rotate[0:70].reshape(70,128,128,1)\n",
    "X_tensor[825:895] = X_test_Zoom_In_1[0:70].reshape(70,128,128,1)\n",
    "X_tensor[895:965] = X_test_Zoom_In_2[0:70].reshape(70,128,128,1)\n",
    "X_tensor[965:1035] = X_test_sketch_AR[0:70].reshape(70,128,128,1)\n",
    "\n",
    "\n",
    "X_tensor[1035:1223] = X_train_CUHK.reshape(188,128,128,1)\n",
    "X_tensor[1223:1411] = X_train_Flip_CUHK.reshape(188,128,128,1)\n",
    "X_tensor[1411:1599] = X_train_Rotate_CUHK.reshape(188,128,128,1)\n",
    "X_tensor[1599:1787] = X_train_Zoom_In_1_CUHK.reshape(188,128,128,1)\n",
    "X_tensor[1787:1975] = X_train_Zoom_In_2_CUHK.reshape(188,128,128,1)\n",
    "X_tensor[1975:2075] = X_test_CUHK[0:100].reshape(100,128,128,1)\n",
    "X_tensor[2075:2175] = X_test_Flip_CUHK[0:100].reshape(100,128,128,1)\n",
    "X_tensor[2175:2275] = X_test_Rotate_CUHK[0:100].reshape(100,128,128,1)\n",
    "X_tensor[2275:2375] = X_test_Zoom_In_1_CUHK[0:100].reshape(100,128,128,1)\n",
    "X_tensor[2375:2475] = X_test_Zoom_In_2_CUHK[0:100].reshape(100,128,128,1)\n",
    "X_tensor[2475:2575] = X_test_sketch_CUHK[0:100].reshape(100,128,128,1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2405, 128, 128, 1)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_tensor.shape"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "X_tensor_CUHK   = np.ndarray(shape=(685, height*width), dtype=np.float64)\n",
    "X_tensor_CUHK = X_tensor_CUHK.reshape(685,128,128,1)\n",
    "X_test_CUHK = X_test_CUHK.reshape(123,128,128,1)\n",
    "\n",
    "X_tensor_CUHK[0:123] = X_train_CUHK.reshape(123,128,128,1)\n",
    "X_tensor_CUHK[123:246] = X_train_Flip_CUHK.reshape(123,128,128,1)\n",
    "X_tensor_CUHK[246:369] = X_train_Rotate_CUHK.reshape(123,128,128,1)\n",
    "X_tensor_CUHK[369:492] = X_train_Zoom_In_1_CUHK.reshape(123,128,128,1)\n",
    "X_tensor_CUHK[492:615] = X_train_Zoom_In_2_CUHK.reshape(123,128,128,1)\n",
    "X_tensor_CUHK[615:685] = X_test_CUHK[0:70]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "y_labels = np.zeros(2405)\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "\n",
    "y_labels[0:123] = np.arange(0,123,1)\n",
    "y_labels[123:246] = np.arange(0,123,1)\n",
    "y_labels[246:369] = np.arange(0,123,1)\n",
    "y_labels[369:492] = np.arange(0,123,1)\n",
    "y_labels[492:615] = np.arange(0,123,1)\n",
    "y_labels[615:685] = np.arange(0,70,1)\n",
    "y_labels[685:755] = np.arange(0,70,1)\n",
    "y_labels[755:825] = np.arange(0,70,1)\n",
    "y_labels[825:895] = np.arange(0,70,1)\n",
    "y_labels[895:965] = np.arange(0,70,1)\n",
    "\n",
    "y_labels[965:1153] = np.arange(123,311,1)\n",
    "y_labels[1153:1341] = np.arange(123,311,1)\n",
    "y_labels[1341:1529] = np.arange(123,311,1)\n",
    "y_labels[1529:1717] = np.arange(123,311,1)\n",
    "y_labels[1717:1905] = np.arange(123,311,1)\n",
    "\n",
    "y_labels[1905:2005] = np.arange(123,223,1)\n",
    "y_labels[2005:2105] = np.arange(123,223,1)\n",
    "y_labels[2105:2205] = np.arange(123,223,1)\n",
    "y_labels[2205:2305] = np.arange(123,223,1)\n",
    "y_labels[2305:2405] = np.arange(123,223,1)\n",
    "\n",
    "\n",
    "y_labels = to_categorical(y_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2405, 128, 128, 1)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = np.zeros(311)\n",
    "y_test = np.arange(0,311,1)\n",
    "\n",
    "y_test = to_categorical(y_test)\n",
    "\n",
    "y_test_1 = y_test[70:123]\n",
    "y_test_2 = y_test[223:311]\n",
    "\n",
    "y_test_enc = np.ndarray(shape = (141, 311))\n",
    "y_test_enc[0:53] = y_test_1\n",
    "y_test_enc[53:141] = y_test_2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(188, 16384)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_CUHK.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_all = np.ndarray(shape = (141, 128*128))\n",
    "X_test_all[0:53] = X_test_AR[70:123]\n",
    "X_test_all[53:141] = X_test_CUHK[100:188]\n",
    "X_test_all = X_test_all.reshape(141,128,128,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2405, 128, 128, 1)\n",
      "(2405, 311)\n",
      "(141, 128, 128, 1)\n",
      "(141, 311)\n"
     ]
    }
   ],
   "source": [
    "print(X_tensor.shape)\n",
    "print(y_labels.shape)\n",
    "print(X_test_all.shape)\n",
    "print(y_test_enc.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First LeNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_21 (Conv2D)           (None, 126, 126, 6)       60        \n",
      "_________________________________________________________________\n",
      "average_pooling2d_1 (Average (None, 63, 63, 6)         0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 63, 63, 6)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_22 (Conv2D)           (None, 61, 61, 16)        880       \n",
      "_________________________________________________________________\n",
      "average_pooling2d_2 (Average (None, 30, 30, 16)        0         \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 30, 30, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_23 (Conv2D)           (None, 28, 28, 32)        4640      \n",
      "_________________________________________________________________\n",
      "average_pooling2d_3 (Average (None, 14, 14, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 14, 14, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_24 (Conv2D)           (None, 12, 12, 64)        18496     \n",
      "_________________________________________________________________\n",
      "average_pooling2d_4 (Average (None, 6, 6, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 2304)              0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 500)               1152500   \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 311)               155811    \n",
      "=================================================================\n",
      "Total params: 1,332,387\n",
      "Trainable params: 1,332,387\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv2D, Flatten, MaxPooling2D, Dropout, AveragePooling2D\n",
    "\n",
    "\n",
    "model = keras.Sequential()\n",
    "model.add(Conv2D(filters=6, kernel_size=(3, 3), activation='relu', input_shape=(128,128,1)))\n",
    "model.add(AveragePooling2D())\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "\n",
    "model.add(Conv2D(filters=16, kernel_size=(3, 3), activation='relu'))\n",
    "model.add(AveragePooling2D())\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "\n",
    "model.add(Conv2D(filters=32, kernel_size=(3, 3), activation='relu'))\n",
    "model.add(AveragePooling2D())\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv2D(filters=64, kernel_size=(3, 3), activation='relu'))\n",
    "model.add(AveragePooling2D())\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(units=500, activation='relu'))\n",
    "\n",
    "#model.add(Dense(units=84, activation='relu'))\n",
    "\n",
    "model.add(Dense(units=311, activation = 'softmax')) #10 yerine 311, bence cok sacma\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/boyaronur/.conda/envs/untitled1/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 2405 samples, validate on 141 samples\n",
      "Epoch 1/50\n",
      "2405/2405 [==============================] - 20s 8ms/step - loss: 5.8247 - acc: 0.0033 - val_loss: 5.7581 - val_acc: 0.0000e+00\n",
      "Epoch 2/50\n",
      "2405/2405 [==============================] - 19s 8ms/step - loss: 5.7204 - acc: 0.0017 - val_loss: 5.8521 - val_acc: 0.0000e+00\n",
      "Epoch 3/50\n",
      "2405/2405 [==============================] - 19s 8ms/step - loss: 5.6828 - acc: 0.0037 - val_loss: 8.8212 - val_acc: 0.0000e+00\n",
      "Epoch 4/50\n",
      "2405/2405 [==============================] - 21s 9ms/step - loss: 5.6644 - acc: 0.0029 - val_loss: 7.2700 - val_acc: 0.0000e+00\n",
      "Epoch 5/50\n",
      "2405/2405 [==============================] - 25s 11ms/step - loss: 5.6451 - acc: 0.0021 - val_loss: 6.0084 - val_acc: 0.0000e+00\n",
      "Epoch 6/50\n",
      "2405/2405 [==============================] - 22s 9ms/step - loss: 5.6390 - acc: 0.0025 - val_loss: 5.9800 - val_acc: 0.0000e+00\n",
      "Epoch 7/50\n",
      "2405/2405 [==============================] - 20s 8ms/step - loss: 5.6344 - acc: 0.0017 - val_loss: 6.0399 - val_acc: 0.0000e+00\n",
      "Epoch 8/50\n",
      "2405/2405 [==============================] - 19s 8ms/step - loss: 5.6303 - acc: 0.0029 - val_loss: 6.2107 - val_acc: 0.0000e+00\n",
      "Epoch 9/50\n",
      "2405/2405 [==============================] - 19s 8ms/step - loss: 5.6302 - acc: 0.0029 - val_loss: 7.2250 - val_acc: 0.0000e+00\n",
      "Epoch 10/50\n",
      "2405/2405 [==============================] - 19s 8ms/step - loss: 5.6261 - acc: 0.0033 - val_loss: 6.8863 - val_acc: 0.0000e+00\n",
      "Epoch 11/50\n",
      "2405/2405 [==============================] - 19s 8ms/step - loss: 5.6267 - acc: 8.3160e-04 - val_loss: 6.2843 - val_acc: 0.0000e+00\n",
      "Epoch 12/50\n",
      "2405/2405 [==============================] - 19s 8ms/step - loss: 5.6267 - acc: 0.0029 - val_loss: 6.1464 - val_acc: 0.0000e+00\n",
      "Epoch 13/50\n",
      "2405/2405 [==============================] - 19s 8ms/step - loss: 5.6253 - acc: 0.0021 - val_loss: 6.2074 - val_acc: 0.0000e+00\n",
      "Epoch 14/50\n",
      "2405/2405 [==============================] - 20s 8ms/step - loss: 5.6231 - acc: 0.0033 - val_loss: 6.2863 - val_acc: 0.0000e+00\n",
      "Epoch 15/50\n",
      "2405/2405 [==============================] - 21s 9ms/step - loss: 5.6216 - acc: 0.0046 - val_loss: 6.9238 - val_acc: 0.0000e+00\n",
      "Epoch 16/50\n",
      "2405/2405 [==============================] - 28s 12ms/step - loss: 5.6185 - acc: 0.0042 - val_loss: 6.7452 - val_acc: 0.0000e+00\n",
      "Epoch 17/50\n",
      "2405/2405 [==============================] - 22s 9ms/step - loss: 5.6145 - acc: 0.0037 - val_loss: 6.5727 - val_acc: 0.0000e+00\n",
      "Epoch 18/50\n",
      "2405/2405 [==============================] - 21s 9ms/step - loss: 5.6068 - acc: 0.0050 - val_loss: 6.9357 - val_acc: 0.0000e+00\n",
      "Epoch 19/50\n",
      "2405/2405 [==============================] - 20s 8ms/step - loss: 5.5947 - acc: 0.0058 - val_loss: 8.3420 - val_acc: 0.0000e+00\n",
      "Epoch 20/50\n",
      "2405/2405 [==============================] - 20s 8ms/step - loss: 5.5950 - acc: 0.0042 - val_loss: 9.1642 - val_acc: 0.0000e+00\n",
      "Epoch 21/50\n",
      "2405/2405 [==============================] - 23s 9ms/step - loss: 5.5863 - acc: 0.0062 - val_loss: 7.2150 - val_acc: 0.0000e+00\n",
      "Epoch 22/50\n",
      "2405/2405 [==============================] - 20s 8ms/step - loss: 5.5501 - acc: 0.0067 - val_loss: 11.7552 - val_acc: 0.0000e+00\n",
      "Epoch 23/50\n",
      "2405/2405 [==============================] - 21s 9ms/step - loss: 5.5906 - acc: 0.0083 - val_loss: 10.5111 - val_acc: 0.0000e+00\n",
      "Epoch 24/50\n",
      "2405/2405 [==============================] - 21s 9ms/step - loss: 5.5499 - acc: 0.0100 - val_loss: 11.3731 - val_acc: 0.0000e+00\n",
      "Epoch 25/50\n",
      "2405/2405 [==============================] - 22s 9ms/step - loss: 5.4153 - acc: 0.0166 - val_loss: 15.2380 - val_acc: 0.0000e+00\n",
      "Epoch 26/50\n",
      "2405/2405 [==============================] - 21s 9ms/step - loss: 5.3522 - acc: 0.0158 - val_loss: 12.3539 - val_acc: 0.0000e+00\n",
      "Epoch 27/50\n",
      "2405/2405 [==============================] - 20s 8ms/step - loss: 5.1789 - acc: 0.0245 - val_loss: 14.7966 - val_acc: 0.0000e+00\n",
      "Epoch 28/50\n",
      "2405/2405 [==============================] - 20s 8ms/step - loss: 5.0049 - acc: 0.0279 - val_loss: 13.5088 - val_acc: 0.0000e+00\n",
      "Epoch 29/50\n",
      "2405/2405 [==============================] - 20s 8ms/step - loss: 4.8005 - acc: 0.0391 - val_loss: 13.7005 - val_acc: 0.0000e+00\n",
      "Epoch 30/50\n",
      "2405/2405 [==============================] - 21s 9ms/step - loss: 4.6122 - acc: 0.0449 - val_loss: 13.3987 - val_acc: 0.0000e+00\n",
      "Epoch 31/50\n",
      "2405/2405 [==============================] - 20s 8ms/step - loss: 4.3872 - acc: 0.0615 - val_loss: 14.7088 - val_acc: 0.0000e+00\n",
      "Epoch 32/50\n",
      "2405/2405 [==============================] - 20s 8ms/step - loss: 4.0809 - acc: 0.0919 - val_loss: 15.0903 - val_acc: 0.0000e+00\n",
      "Epoch 33/50\n",
      "2405/2405 [==============================] - 20s 8ms/step - loss: 3.8911 - acc: 0.1173 - val_loss: 14.5760 - val_acc: 0.0000e+00\n",
      "Epoch 34/50\n",
      "2405/2405 [==============================] - 20s 8ms/step - loss: 3.5811 - acc: 0.1701 - val_loss: 15.3661 - val_acc: 0.0000e+00\n",
      "Epoch 35/50\n",
      "2405/2405 [==============================] - 21s 9ms/step - loss: 3.2664 - acc: 0.2129 - val_loss: 14.8823 - val_acc: 0.0000e+00\n",
      "Epoch 36/50\n",
      "2405/2405 [==============================] - 22s 9ms/step - loss: 3.0135 - acc: 0.2715 - val_loss: 15.2634 - val_acc: 0.0000e+00\n",
      "Epoch 37/50\n",
      "2405/2405 [==============================] - 21s 9ms/step - loss: 2.7786 - acc: 0.3189 - val_loss: 15.5771 - val_acc: 0.0000e+00\n",
      "Epoch 38/50\n",
      "2405/2405 [==============================] - 22s 9ms/step - loss: 2.5704 - acc: 0.3630 - val_loss: 14.6185 - val_acc: 0.0000e+00\n",
      "Epoch 39/50\n",
      "2405/2405 [==============================] - 21s 9ms/step - loss: 2.3246 - acc: 0.4141 - val_loss: 14.8642 - val_acc: 0.0000e+00\n",
      "Epoch 40/50\n",
      "2405/2405 [==============================] - 20s 8ms/step - loss: 2.1683 - acc: 0.4624 - val_loss: 15.2738 - val_acc: 0.0000e+00\n",
      "Epoch 41/50\n",
      "2405/2405 [==============================] - 21s 9ms/step - loss: 2.0585 - acc: 0.4894 - val_loss: 15.0367 - val_acc: 0.0000e+00\n",
      "Epoch 42/50\n",
      "2405/2405 [==============================] - 22s 9ms/step - loss: 1.8462 - acc: 0.5405 - val_loss: 15.3484 - val_acc: 0.0000e+00\n",
      "Epoch 43/50\n",
      "2405/2405 [==============================] - 21s 9ms/step - loss: 1.7243 - acc: 0.5755 - val_loss: 15.3426 - val_acc: 0.0000e+00\n",
      "Epoch 44/50\n",
      "2405/2405 [==============================] - 23s 9ms/step - loss: 1.6327 - acc: 0.5954 - val_loss: 15.5790 - val_acc: 0.0000e+00\n",
      "Epoch 45/50\n",
      "2405/2405 [==============================] - 21s 9ms/step - loss: 1.6411 - acc: 0.5979 - val_loss: 15.1165 - val_acc: 0.0000e+00\n",
      "Epoch 46/50\n",
      "2405/2405 [==============================] - 20s 8ms/step - loss: 1.3862 - acc: 0.6628 - val_loss: 15.1208 - val_acc: 0.0000e+00\n",
      "Epoch 47/50\n",
      "2405/2405 [==============================] - 20s 8ms/step - loss: 1.3190 - acc: 0.6873 - val_loss: 15.3053 - val_acc: 0.0000e+00\n",
      "Epoch 48/50\n",
      "2405/2405 [==============================] - 20s 8ms/step - loss: 1.2955 - acc: 0.6865 - val_loss: 15.3412 - val_acc: 0.0000e+00\n",
      "Epoch 49/50\n",
      "2405/2405 [==============================] - 20s 8ms/step - loss: 1.2501 - acc: 0.7019 - val_loss: 15.4134 - val_acc: 0.0000e+00\n",
      "Epoch 50/50\n",
      "2405/2405 [==============================] - 20s 8ms/step - loss: 1.1631 - acc: 0.7222 - val_loss: 15.5539 - val_acc: 0.0000e+00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1c5c6f6400>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss=\"categorical_crossentropy\",optimizer=keras.optimizers.Adam(),metrics=['accuracy'])\n",
    "model.fit(X_tensor, y_labels, validation_data=(X_test_all, y_test_enc), epochs=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ALEX NET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/boyaronur/.conda/envs/untitled1/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_16 (Conv2D)           (None, 54, 54, 96)        11712     \n",
      "_________________________________________________________________\n",
      "activation_11 (Activation)   (None, 54, 54, 96)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_11 (MaxPooling (None, 27, 27, 96)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_17 (Conv2D)           (None, 17, 17, 256)       2973952   \n",
      "_________________________________________________________________\n",
      "activation_12 (Activation)   (None, 17, 17, 256)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_12 (MaxPooling (None, 8, 8, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_18 (Conv2D)           (None, 6, 6, 384)         885120    \n",
      "_________________________________________________________________\n",
      "activation_13 (Activation)   (None, 6, 6, 384)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_19 (Conv2D)           (None, 4, 4, 384)         1327488   \n",
      "_________________________________________________________________\n",
      "activation_14 (Activation)   (None, 4, 4, 384)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_20 (Conv2D)           (None, 2, 2, 256)         884992    \n",
      "_________________________________________________________________\n",
      "activation_15 (Activation)   (None, 2, 2, 256)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_13 (MaxPooling (None, 1, 1, 256)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 4096)              1052672   \n",
      "_________________________________________________________________\n",
      "activation_16 (Activation)   (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 4096)              16781312  \n",
      "_________________________________________________________________\n",
      "activation_17 (Activation)   (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1000)              4097000   \n",
      "_________________________________________________________________\n",
      "activation_18 (Activation)   (None, 1000)              0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 1000)              0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 17)                17017     \n",
      "_________________________________________________________________\n",
      "activation_19 (Activation)   (None, 17)                0         \n",
      "=================================================================\n",
      "Total params: 28,031,265\n",
      "Trainable params: 28,031,265\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout, Flatten, Conv2D, MaxPooling2D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "import numpy as np\n",
    "np.random.seed(1000)\n",
    "#Instantiate an empty model\n",
    "model = Sequential()\n",
    "\n",
    "# 1st Convolutional Layer\n",
    "model.add(Conv2D(filters=96, input_shape=(224,224,1), kernel_size=(11,11), strides=(4,4), padding='valid'))\n",
    "model.add(Activation('relu'))\n",
    "# Max Pooling\n",
    "model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2), padding='valid'))\n",
    "\n",
    "# 2nd Convolutional Layer\n",
    "model.add(Conv2D(filters=256, kernel_size=(11,11), strides=(1,1), padding='valid'))\n",
    "model.add(Activation('relu'))\n",
    "# Max Pooling\n",
    "model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2), padding='valid'))\n",
    "\n",
    "# 3rd Convolutional Layer\n",
    "model.add(Conv2D(filters=384, kernel_size=(3,3), strides=(1,1), padding='valid'))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "# 4th Convolutional Layer\n",
    "model.add(Conv2D(filters=384, kernel_size=(3,3), strides=(1,1), padding='valid'))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "# 5th Convolutional Layer\n",
    "model.add(Conv2D(filters=256, kernel_size=(3,3), strides=(1,1), padding='valid'))\n",
    "model.add(Activation('relu'))\n",
    "# Max Pooling\n",
    "model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2), padding='valid'))\n",
    "\n",
    "# Passing it to a Fully Connected layer\n",
    "model.add(Flatten())\n",
    "# 1st Fully Connected Layer\n",
    "model.add(Dense(4096, input_shape=(224*224*3,)))\n",
    "model.add(Activation('relu'))\n",
    "# Add Dropout to prevent overfitting\n",
    "model.add(Dropout(0.4))\n",
    "\n",
    "# 2nd Fully Connected Layer\n",
    "model.add(Dense(4096))\n",
    "model.add(Activation('relu'))\n",
    "# Add Dropout\n",
    "model.add(Dropout(0.4))\n",
    "\n",
    "# 3rd Fully Connected Layer\n",
    "model.add(Dense(1000))\n",
    "model.add(Activation('relu'))\n",
    "# Add Dropout\n",
    "model.add(Dropout(0.4))\n",
    "\n",
    "# Output Layer\n",
    "model.add(Dense(17))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss=keras.losses.categorical_crossentropy, optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LeNet Original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_2 (Conv2D)            (None, 126, 126, 6)       60        \n",
      "_________________________________________________________________\n",
      "average_pooling2d_1 (Average (None, 63, 63, 6)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 61, 61, 16)        880       \n",
      "_________________________________________________________________\n",
      "average_pooling2d_2 (Average (None, 30, 30, 16)        0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 14400)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 120)               1728120   \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 100)               12100     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 311)               31411     \n",
      "=================================================================\n",
      "Total params: 1,772,571\n",
      "Trainable params: 1,772,571\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout, Flatten, Conv2D, MaxPooling2D, AveragePooling2D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "model = keras.Sequential()\n",
    "\n",
    "model.add(Conv2D(filters=6, kernel_size=(3, 3), activation='relu', input_shape=(128,128,1)))\n",
    "model.add(AveragePooling2D())\n",
    "\n",
    "model.add(Conv2D(filters=16, kernel_size=(3, 3), activation='relu'))\n",
    "model.add(AveragePooling2D())\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(units=120, activation='relu'))\n",
    "\n",
    "model.add(Dense(units=100, activation='relu'))\n",
    "\n",
    "model.add(Dense(units=311, activation = 'softmax'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/boyaronur/.conda/envs/untitled1/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 2405 samples, validate on 141 samples\n",
      "Epoch 1/50\n",
      "2405/2405 [==============================] - 16s 7ms/step - loss: 6.0236 - acc: 8.3160e-04 - val_loss: 7.2781 - val_acc: 0.0071\n",
      "Epoch 2/50\n",
      "2405/2405 [==============================] - 14s 6ms/step - loss: 5.7316 - acc: 0.0046 - val_loss: 6.2208 - val_acc: 0.0000e+00\n",
      "Epoch 3/50\n",
      "2405/2405 [==============================] - 15s 6ms/step - loss: 5.6759 - acc: 0.0054 - val_loss: 7.2403 - val_acc: 0.0000e+00\n",
      "Epoch 4/50\n",
      "2405/2405 [==============================] - 21s 9ms/step - loss: 5.5868 - acc: 0.0166 - val_loss: 10.5160 - val_acc: 0.0000e+00\n",
      "Epoch 5/50\n",
      "2405/2405 [==============================] - 16s 7ms/step - loss: 5.3390 - acc: 0.0249 - val_loss: 11.9564 - val_acc: 0.0000e+00\n",
      "Epoch 6/50\n",
      "2405/2405 [==============================] - 16s 7ms/step - loss: 4.9644 - acc: 0.0370 - val_loss: 13.1284 - val_acc: 0.0071\n",
      "Epoch 7/50\n",
      "2405/2405 [==============================] - 16s 7ms/step - loss: 4.6200 - acc: 0.0478 - val_loss: 13.5456 - val_acc: 0.0213\n",
      "Epoch 8/50\n",
      "2405/2405 [==============================] - 17s 7ms/step - loss: 4.2683 - acc: 0.0807 - val_loss: 14.2002 - val_acc: 0.0355\n",
      "Epoch 9/50\n",
      "2405/2405 [==============================] - 17s 7ms/step - loss: 3.9095 - acc: 0.1102 - val_loss: 15.3345 - val_acc: 0.0142\n",
      "Epoch 10/50\n",
      "2405/2405 [==============================] - 15s 6ms/step - loss: 3.5411 - acc: 0.1672 - val_loss: 15.7456 - val_acc: 0.0142\n",
      "Epoch 11/50\n",
      "2405/2405 [==============================] - 15s 6ms/step - loss: 3.2502 - acc: 0.2279 - val_loss: 15.7452 - val_acc: 0.0000e+00\n",
      "Epoch 12/50\n",
      "2405/2405 [==============================] - 15s 6ms/step - loss: 2.9594 - acc: 0.2952 - val_loss: 15.4534 - val_acc: 0.0213\n",
      "Epoch 13/50\n",
      "2405/2405 [==============================] - 17s 7ms/step - loss: 2.5497 - acc: 0.3830 - val_loss: 15.4302 - val_acc: 0.0355\n",
      "Epoch 14/50\n",
      "2405/2405 [==============================] - 16s 7ms/step - loss: 2.2157 - acc: 0.4599 - val_loss: 15.1267 - val_acc: 0.0426\n",
      "Epoch 15/50\n",
      "2405/2405 [==============================] - 15s 6ms/step - loss: 1.9490 - acc: 0.5401 - val_loss: 15.7999 - val_acc: 0.0142\n",
      "Epoch 16/50\n",
      "2405/2405 [==============================] - 15s 6ms/step - loss: 1.7324 - acc: 0.6000 - val_loss: 15.9124 - val_acc: 0.0000e+00\n",
      "Epoch 17/50\n",
      "2405/2405 [==============================] - 15s 6ms/step - loss: 1.5206 - acc: 0.6486 - val_loss: 15.6856 - val_acc: 0.0142\n",
      "Epoch 18/50\n",
      "2405/2405 [==============================] - 16s 7ms/step - loss: 1.2875 - acc: 0.7272 - val_loss: 15.5749 - val_acc: 0.0284\n",
      "Epoch 19/50\n",
      "2405/2405 [==============================] - 15s 6ms/step - loss: 1.2598 - acc: 0.7484 - val_loss: 15.9508 - val_acc: 0.0071\n",
      "Epoch 20/50\n",
      "2405/2405 [==============================] - 17s 7ms/step - loss: 1.3108 - acc: 0.7547 - val_loss: 15.5644 - val_acc: 0.0142\n",
      "Epoch 21/50\n",
      "2405/2405 [==============================] - 16s 7ms/step - loss: 1.1386 - acc: 0.7875 - val_loss: 16.0038 - val_acc: 0.0071\n",
      "Epoch 22/50\n",
      "2405/2405 [==============================] - 20s 8ms/step - loss: 1.0503 - acc: 0.8021 - val_loss: 15.9462 - val_acc: 0.0071\n",
      "Epoch 23/50\n",
      "2405/2405 [==============================] - 15s 6ms/step - loss: 0.9930 - acc: 0.8237 - val_loss: 16.0723 - val_acc: 0.0000e+00\n",
      "Epoch 24/50\n",
      "2405/2405 [==============================] - 16s 7ms/step - loss: 0.9733 - acc: 0.8121 - val_loss: 16.1181 - val_acc: 0.0000e+00\n",
      "Epoch 25/50\n",
      "2405/2405 [==============================] - 16s 7ms/step - loss: 1.0390 - acc: 0.8112 - val_loss: 16.0158 - val_acc: 0.0000e+00\n",
      "Epoch 26/50\n",
      "2405/2405 [==============================] - 15s 6ms/step - loss: 0.9058 - acc: 0.8441 - val_loss: 16.0410 - val_acc: 0.0000e+00\n",
      "Epoch 27/50\n",
      "2405/2405 [==============================] - 16s 7ms/step - loss: 0.8085 - acc: 0.8603 - val_loss: 16.1181 - val_acc: 0.0000e+00\n",
      "Epoch 28/50\n",
      "2405/2405 [==============================] - 18s 7ms/step - loss: 0.7679 - acc: 0.8711 - val_loss: 16.1181 - val_acc: 0.0000e+00\n",
      "Epoch 29/50\n",
      "2405/2405 [==============================] - 21s 9ms/step - loss: 0.7999 - acc: 0.8657 - val_loss: 16.0740 - val_acc: 0.0000e+00\n",
      "Epoch 30/50\n",
      "2405/2405 [==============================] - 16s 7ms/step - loss: 0.8045 - acc: 0.8565 - val_loss: 16.0304 - val_acc: 0.0000e+00\n",
      "Epoch 31/50\n",
      "2405/2405 [==============================] - 17s 7ms/step - loss: 0.7672 - acc: 0.8719 - val_loss: 16.1181 - val_acc: 0.0000e+00\n",
      "Epoch 32/50\n",
      "2405/2405 [==============================] - 18s 8ms/step - loss: 0.7383 - acc: 0.8757 - val_loss: 16.0151 - val_acc: 0.0000e+00\n",
      "Epoch 33/50\n",
      "2405/2405 [==============================] - 20s 8ms/step - loss: 0.7288 - acc: 0.8719 - val_loss: 16.0189 - val_acc: 0.0000e+00\n",
      "Epoch 34/50\n",
      "2405/2405 [==============================] - 17s 7ms/step - loss: 0.7342 - acc: 0.8711 - val_loss: 16.1181 - val_acc: 0.0000e+00\n",
      "Epoch 35/50\n",
      "2405/2405 [==============================] - 20s 8ms/step - loss: 0.6842 - acc: 0.8782 - val_loss: 16.0038 - val_acc: 0.0071\n",
      "Epoch 36/50\n",
      "2405/2405 [==============================] - 17s 7ms/step - loss: 0.6731 - acc: 0.8811 - val_loss: 16.0542 - val_acc: 0.0000e+00\n",
      "Epoch 37/50\n",
      "2405/2405 [==============================] - 18s 7ms/step - loss: 0.6643 - acc: 0.8836 - val_loss: 16.0045 - val_acc: 0.0071\n",
      "Epoch 38/50\n",
      "2405/2405 [==============================] - 16s 7ms/step - loss: 0.6587 - acc: 0.8836 - val_loss: 16.0211 - val_acc: 0.0000e+00\n",
      "Epoch 39/50\n",
      "2405/2405 [==============================] - 17s 7ms/step - loss: 0.6582 - acc: 0.8832 - val_loss: 16.0040 - val_acc: 0.0071\n",
      "Epoch 40/50\n",
      "2405/2405 [==============================] - 21s 9ms/step - loss: 0.6574 - acc: 0.8832 - val_loss: 16.1142 - val_acc: 0.0000e+00\n",
      "Epoch 41/50\n",
      "2405/2405 [==============================] - 27s 11ms/step - loss: 0.6595 - acc: 0.8832 - val_loss: 16.0980 - val_acc: 0.0000e+00\n",
      "Epoch 42/50\n",
      "2405/2405 [==============================] - 30s 12ms/step - loss: 0.6584 - acc: 0.8832 - val_loss: 16.1181 - val_acc: 0.0000e+00\n",
      "Epoch 43/50\n",
      "2405/2405 [==============================] - 27s 11ms/step - loss: 0.6543 - acc: 0.8832 - val_loss: 16.1122 - val_acc: 0.0000e+00\n",
      "Epoch 44/50\n",
      "2405/2405 [==============================] - 31s 13ms/step - loss: 0.6549 - acc: 0.8832 - val_loss: 16.0783 - val_acc: 0.0000e+00\n",
      "Epoch 45/50\n",
      "2405/2405 [==============================] - 28s 12ms/step - loss: 0.6576 - acc: 0.8840 - val_loss: 16.0431 - val_acc: 0.0000e+00\n",
      "Epoch 46/50\n",
      "2405/2405 [==============================] - 28s 12ms/step - loss: 0.6562 - acc: 0.8836 - val_loss: 16.0319 - val_acc: 0.0000e+00\n",
      "Epoch 47/50\n",
      "2405/2405 [==============================] - 30s 13ms/step - loss: 0.6552 - acc: 0.8836 - val_loss: 16.0387 - val_acc: 0.0000e+00\n",
      "Epoch 48/50\n",
      "2405/2405 [==============================] - 26s 11ms/step - loss: 0.6543 - acc: 0.8836 - val_loss: 16.0710 - val_acc: 0.0000e+00\n",
      "Epoch 49/50\n",
      "2405/2405 [==============================] - 32s 13ms/step - loss: 0.6552 - acc: 0.8832 - val_loss: 16.0048 - val_acc: 0.0071\n",
      "Epoch 50/50\n",
      "2405/2405 [==============================] - 27s 11ms/step - loss: 0.6538 - acc: 0.8832 - val_loss: 16.0155 - val_acc: 0.0000e+00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1c35ff99e8>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss=\"categorical_crossentropy\",optimizer=keras.optimizers.Adam(),metrics=['accuracy'])\n",
    "model.fit(X_tensor, y_labels, validation_data=(X_test_all, y_test_enc), epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"categorical_crossentropy\",optimizer=keras.optimizers.Adam(),metrics=['accuracy'])\n",
    "model.fit(X_tensor, y_labels, validation_data=(X_test_all, y_test_enc), epochs=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LeNet with Regularization, Dropout and MaxPooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/boyaronur/.conda/envs/untitled1/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /Users/boyaronur/.conda/envs/untitled1/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 126, 126, 6)       60        \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 63, 63, 6)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 61, 61, 16)        880       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 30, 30, 16)        0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 30, 30, 16)        0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 14400)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 120)               1728120   \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 100)               12100     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 311)               31411     \n",
      "=================================================================\n",
      "Total params: 1,772,571\n",
      "Trainable params: 1,772,571\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout, Flatten, Conv2D, MaxPooling2D, AveragePooling2D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "import numpy as np\n",
    "from keras.regularizers import l2\n",
    "\n",
    "\n",
    "\n",
    "model = keras.Sequential()\n",
    "\n",
    "model.add(Conv2D(filters=6, kernel_size=(3, 3),kernel_regularizer=l2(0.01), bias_regularizer=l2(0.01), activation='relu', input_shape=(128,128,1)))\n",
    "model.add(MaxPooling2D(2,2))\n",
    "\n",
    "model.add(Conv2D(filters=16, kernel_size=(3, 3),kernel_regularizer=l2(0.01), bias_regularizer=l2(0.01), activation='relu'))\n",
    "model.add(MaxPooling2D(2,2))\n",
    "model.add(Dropout(0.05))\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(units=120, activation='relu'))\n",
    "\n",
    "model.add(Dense(units=100, activation='relu'))\n",
    "\n",
    "model.add(Dense(units=311, activation = 'softmax'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/boyaronur/.conda/envs/untitled1/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 2405 samples, validate on 141 samples\n",
      "Epoch 1/50\n",
      "2405/2405 [==============================] - 38s 16ms/step - loss: 6.0851 - acc: 0.0029 - val_loss: 6.2897 - val_acc: 0.0000e+00\n",
      "Epoch 2/50\n",
      "2405/2405 [==============================] - 34s 14ms/step - loss: 5.7732 - acc: 0.0017 - val_loss: 5.9970 - val_acc: 0.0000e+00\n",
      "Epoch 3/50\n",
      "2405/2405 [==============================] - 33s 14ms/step - loss: 5.7089 - acc: 0.0050 - val_loss: 6.3799 - val_acc: 0.0000e+00\n",
      "Epoch 4/50\n",
      "2405/2405 [==============================] - 32s 13ms/step - loss: 5.6339 - acc: 0.0100 - val_loss: 7.4315 - val_acc: 0.0000e+00\n",
      "Epoch 5/50\n",
      "2405/2405 [==============================] - 30s 12ms/step - loss: 5.5618 - acc: 0.0175 - val_loss: 9.5051 - val_acc: 0.0000e+00\n",
      "Epoch 6/50\n",
      "2405/2405 [==============================] - 33s 14ms/step - loss: 5.4012 - acc: 0.0299 - val_loss: 14.8212 - val_acc: 0.0000e+00\n",
      "Epoch 7/50\n",
      "2405/2405 [==============================] - 31s 13ms/step - loss: 5.1054 - acc: 0.0337 - val_loss: 15.2703 - val_acc: 0.0000e+00\n",
      "Epoch 8/50\n",
      "2405/2405 [==============================] - 29s 12ms/step - loss: 4.7243 - acc: 0.0466 - val_loss: 15.6139 - val_acc: 0.0071\n",
      "Epoch 9/50\n",
      "2405/2405 [==============================] - 28s 11ms/step - loss: 4.3430 - acc: 0.0761 - val_loss: 15.5105 - val_acc: 0.0071\n",
      "Epoch 10/50\n",
      "2405/2405 [==============================] - 32s 13ms/step - loss: 3.9626 - acc: 0.1214 - val_loss: 15.5053 - val_acc: 0.0355\n",
      "Epoch 11/50\n",
      "2405/2405 [==============================] - 27s 11ms/step - loss: 3.5822 - acc: 0.1917 - val_loss: 15.4811 - val_acc: 0.0284\n",
      "Epoch 12/50\n",
      "2405/2405 [==============================] - 26s 11ms/step - loss: 3.1300 - acc: 0.2919 - val_loss: 15.6792 - val_acc: 0.0213\n",
      "Epoch 13/50\n",
      "2405/2405 [==============================] - 28s 11ms/step - loss: 2.6625 - acc: 0.3942 - val_loss: 15.4393 - val_acc: 0.0355\n",
      "Epoch 14/50\n",
      "2405/2405 [==============================] - 27s 11ms/step - loss: 2.3648 - acc: 0.4898 - val_loss: 15.5782 - val_acc: 0.0213\n",
      "Epoch 15/50\n",
      "2405/2405 [==============================] - 27s 11ms/step - loss: 2.0861 - acc: 0.5630 - val_loss: 16.0890 - val_acc: 0.0071\n",
      "Epoch 16/50\n",
      "2405/2405 [==============================] - 27s 11ms/step - loss: 1.7014 - acc: 0.6686 - val_loss: 15.9447 - val_acc: 0.0142\n",
      "Epoch 17/50\n",
      "2405/2405 [==============================] - 27s 11ms/step - loss: 1.5249 - acc: 0.7168 - val_loss: 15.8859 - val_acc: 0.0142\n",
      "Epoch 18/50\n",
      "2405/2405 [==============================] - 27s 11ms/step - loss: 1.3313 - acc: 0.7684 - val_loss: 15.9864 - val_acc: 0.0142\n",
      "Epoch 19/50\n",
      "2405/2405 [==============================] - 27s 11ms/step - loss: 1.3078 - acc: 0.7830 - val_loss: 15.9288 - val_acc: 0.0142\n",
      "Epoch 20/50\n",
      "2405/2405 [==============================] - 29s 12ms/step - loss: 1.3873 - acc: 0.7568 - val_loss: 15.9932 - val_acc: 0.0142\n",
      "Epoch 21/50\n",
      "2405/2405 [==============================] - 29s 12ms/step - loss: 1.2706 - acc: 0.8096 - val_loss: 15.7665 - val_acc: 0.0284\n",
      "Epoch 22/50\n",
      "2405/2405 [==============================] - 29s 12ms/step - loss: 1.2159 - acc: 0.8179 - val_loss: 15.9974 - val_acc: 0.0142\n",
      "Epoch 23/50\n",
      "2405/2405 [==============================] - 24s 10ms/step - loss: 1.3864 - acc: 0.7771 - val_loss: 15.4281 - val_acc: 0.0496\n",
      "Epoch 24/50\n",
      "2405/2405 [==============================] - 15s 6ms/step - loss: 1.3957 - acc: 0.7954 - val_loss: 15.7716 - val_acc: 0.0284\n",
      "Epoch 25/50\n",
      "2405/2405 [==============================] - 15s 6ms/step - loss: 1.2475 - acc: 0.8283 - val_loss: 16.0570 - val_acc: 0.0000e+00\n",
      "Epoch 26/50\n",
      "2405/2405 [==============================] - 16s 6ms/step - loss: 1.1580 - acc: 0.8407 - val_loss: 16.1182 - val_acc: 0.0071\n",
      "Epoch 27/50\n",
      "2405/2405 [==============================] - 15s 6ms/step - loss: 1.1124 - acc: 0.8424 - val_loss: 16.1183 - val_acc: 0.0071\n",
      "Epoch 28/50\n",
      "2405/2405 [==============================] - 16s 7ms/step - loss: 1.1173 - acc: 0.8412 - val_loss: 15.7760 - val_acc: 0.0284\n",
      "Epoch 29/50\n",
      "2405/2405 [==============================] - 16s 6ms/step - loss: 1.0922 - acc: 0.8532 - val_loss: 15.8191 - val_acc: 0.0213\n",
      "Epoch 30/50\n",
      "2405/2405 [==============================] - 15s 6ms/step - loss: 1.0700 - acc: 0.8549 - val_loss: 15.9882 - val_acc: 0.0142\n",
      "Epoch 31/50\n",
      "2405/2405 [==============================] - 15s 6ms/step - loss: 1.0729 - acc: 0.8578 - val_loss: 15.8880 - val_acc: 0.0213\n",
      "Epoch 32/50\n",
      "2405/2405 [==============================] - 16s 7ms/step - loss: 1.0488 - acc: 0.8657 - val_loss: 16.0010 - val_acc: 0.0142\n",
      "Epoch 33/50\n",
      "2405/2405 [==============================] - 15s 6ms/step - loss: 1.0331 - acc: 0.8657 - val_loss: 16.0589 - val_acc: 0.0071\n",
      "Epoch 34/50\n",
      "2405/2405 [==============================] - 16s 7ms/step - loss: 1.0177 - acc: 0.8690 - val_loss: 15.8840 - val_acc: 0.0213\n",
      "Epoch 35/50\n",
      "2405/2405 [==============================] - 15s 6ms/step - loss: 1.0369 - acc: 0.8636 - val_loss: 15.4671 - val_acc: 0.0426\n",
      "Epoch 36/50\n",
      "2405/2405 [==============================] - 16s 7ms/step - loss: 1.1305 - acc: 0.8420 - val_loss: 15.7714 - val_acc: 0.0284\n",
      "Epoch 37/50\n",
      "2405/2405 [==============================] - 15s 6ms/step - loss: 1.1571 - acc: 0.8333 - val_loss: 15.8863 - val_acc: 0.0213\n",
      "Epoch 38/50\n",
      "2405/2405 [==============================] - 15s 6ms/step - loss: 1.0717 - acc: 0.8565 - val_loss: 15.8961 - val_acc: 0.0142\n",
      "Epoch 39/50\n",
      "2405/2405 [==============================] - 15s 6ms/step - loss: 1.0223 - acc: 0.8644 - val_loss: 16.1151 - val_acc: 0.0071\n",
      "Epoch 40/50\n",
      "2405/2405 [==============================] - 16s 7ms/step - loss: 1.1352 - acc: 0.8366 - val_loss: 15.8866 - val_acc: 0.0213\n",
      "Epoch 41/50\n",
      "2405/2405 [==============================] - 15s 6ms/step - loss: 1.0288 - acc: 0.8628 - val_loss: 16.0003 - val_acc: 0.0142\n",
      "Epoch 42/50\n",
      "2405/2405 [==============================] - 16s 7ms/step - loss: 0.9929 - acc: 0.8707 - val_loss: 15.9988 - val_acc: 0.0142\n",
      "Epoch 43/50\n",
      "2405/2405 [==============================] - 16s 7ms/step - loss: 0.9862 - acc: 0.8703 - val_loss: 15.9970 - val_acc: 0.0142\n",
      "Epoch 44/50\n",
      "2405/2405 [==============================] - 16s 7ms/step - loss: 0.9739 - acc: 0.8703 - val_loss: 15.9952 - val_acc: 0.0142\n",
      "Epoch 45/50\n",
      "2405/2405 [==============================] - 16s 7ms/step - loss: 0.9810 - acc: 0.8682 - val_loss: 15.9250 - val_acc: 0.0142\n",
      "Epoch 46/50\n",
      "2405/2405 [==============================] - 16s 7ms/step - loss: 0.9819 - acc: 0.8690 - val_loss: 15.8778 - val_acc: 0.0213\n",
      "Epoch 47/50\n",
      "2405/2405 [==============================] - 16s 6ms/step - loss: 1.0067 - acc: 0.8661 - val_loss: 15.9415 - val_acc: 0.0142\n",
      "Epoch 48/50\n",
      "2405/2405 [==============================] - 16s 7ms/step - loss: 1.0267 - acc: 0.8653 - val_loss: 15.6473 - val_acc: 0.0355\n",
      "Epoch 49/50\n",
      "2405/2405 [==============================] - 16s 6ms/step - loss: 1.0075 - acc: 0.8694 - val_loss: 15.8742 - val_acc: 0.0213\n",
      "Epoch 50/50\n",
      "2405/2405 [==============================] - 16s 7ms/step - loss: 0.9984 - acc: 0.8682 - val_loss: 15.8134 - val_acc: 0.0213\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1c1e4cd1d0>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss=\"categorical_crossentropy\",optimizer=keras.optimizers.Adam(),metrics=['accuracy'])\n",
    "model.fit(X_tensor, y_labels, validation_data=(X_test_all, y_test_enc), epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2405 samples, validate on 141 samples\n",
      "Epoch 1/150\n",
      "2405/2405 [==============================] - 34s 14ms/step - loss: 0.9951 - acc: 0.8694 - val_loss: 15.8706 - val_acc: 0.0213\n",
      "Epoch 2/150\n",
      "2405/2405 [==============================] - 41s 17ms/step - loss: 0.9945 - acc: 0.8682 - val_loss: 15.7545 - val_acc: 0.0284\n",
      "Epoch 3/150\n",
      "2405/2405 [==============================] - 32s 13ms/step - loss: 0.9917 - acc: 0.8694 - val_loss: 15.8670 - val_acc: 0.0213\n",
      "Epoch 4/150\n",
      "2405/2405 [==============================] - 31s 13ms/step - loss: 0.9903 - acc: 0.8686 - val_loss: 15.8653 - val_acc: 0.0213\n",
      "Epoch 5/150\n",
      "2405/2405 [==============================] - 32s 13ms/step - loss: 0.9864 - acc: 0.8690 - val_loss: 15.8636 - val_acc: 0.0213\n",
      "Epoch 6/150\n",
      "2405/2405 [==============================] - 31s 13ms/step - loss: 0.9845 - acc: 0.8690 - val_loss: 15.8620 - val_acc: 0.0213\n",
      "Epoch 7/150\n",
      "2405/2405 [==============================] - 35s 15ms/step - loss: 1.0134 - acc: 0.8615 - val_loss: 15.8632 - val_acc: 0.0213\n",
      "Epoch 8/150\n",
      "2405/2405 [==============================] - 33s 14ms/step - loss: 1.1680 - acc: 0.8195 - val_loss: 15.8646 - val_acc: 0.0213\n",
      "Epoch 9/150\n",
      "2405/2405 [==============================] - 38s 16ms/step - loss: 1.1235 - acc: 0.8345 - val_loss: 15.8355 - val_acc: 0.0213\n",
      "Epoch 10/150\n",
      "2405/2405 [==============================] - 31s 13ms/step - loss: 1.0230 - acc: 0.8603 - val_loss: 15.5211 - val_acc: 0.0426\n",
      "Epoch 11/150\n",
      "2405/2405 [==============================] - 30s 13ms/step - loss: 1.0374 - acc: 0.8590 - val_loss: 15.9008 - val_acc: 0.0142\n",
      "Epoch 12/150\n",
      "2405/2405 [==============================] - 31s 13ms/step - loss: 1.0053 - acc: 0.8657 - val_loss: 15.8302 - val_acc: 0.0213\n",
      "Epoch 13/150\n",
      "2405/2405 [==============================] - 30s 12ms/step - loss: 0.9981 - acc: 0.8678 - val_loss: 15.5689 - val_acc: 0.0355\n",
      "Epoch 14/150\n",
      "2405/2405 [==============================] - 32s 13ms/step - loss: 0.9932 - acc: 0.8674 - val_loss: 15.6299 - val_acc: 0.0355\n",
      "Epoch 15/150\n",
      "2405/2405 [==============================] - 46s 19ms/step - loss: 0.9926 - acc: 0.8682 - val_loss: 15.7426 - val_acc: 0.0284\n",
      "Epoch 16/150\n",
      "2405/2405 [==============================] - 46s 19ms/step - loss: 1.0710 - acc: 0.8470 - val_loss: 15.9736 - val_acc: 0.0142\n",
      "Epoch 17/150\n",
      "2405/2405 [==============================] - 44s 18ms/step - loss: 1.0993 - acc: 0.8437 - val_loss: 15.8319 - val_acc: 0.0213\n",
      "Epoch 18/150\n",
      "2405/2405 [==============================] - 45s 19ms/step - loss: 1.0508 - acc: 0.8599 - val_loss: 15.4044 - val_acc: 0.0496\n",
      "Epoch 19/150\n",
      "2405/2405 [==============================] - 46s 19ms/step - loss: 1.1307 - acc: 0.8387 - val_loss: 15.6329 - val_acc: 0.0355\n",
      "Epoch 20/150\n",
      "2405/2405 [==============================] - 43s 18ms/step - loss: 1.1030 - acc: 0.8445 - val_loss: 15.6399 - val_acc: 0.0284\n",
      "Epoch 21/150\n",
      "2405/2405 [==============================] - 44s 18ms/step - loss: 1.0980 - acc: 0.8432 - val_loss: 14.2181 - val_acc: 0.1206\n",
      "Epoch 22/150\n",
      "2405/2405 [==============================] - 51s 21ms/step - loss: 1.2053 - acc: 0.8158 - val_loss: 15.7504 - val_acc: 0.0284\n",
      "Epoch 23/150\n",
      "2405/2405 [==============================] - 43s 18ms/step - loss: 1.1907 - acc: 0.8245 - val_loss: 16.0953 - val_acc: 0.0071\n",
      "Epoch 24/150\n",
      "2405/2405 [==============================] - 43s 18ms/step - loss: 1.0901 - acc: 0.8570 - val_loss: 16.2089 - val_acc: 0.0000e+00\n",
      "Epoch 25/150\n",
      "2405/2405 [==============================] - 42s 17ms/step - loss: 1.0593 - acc: 0.8640 - val_loss: 15.9794 - val_acc: 0.0142\n",
      "Epoch 26/150\n",
      "2405/2405 [==============================] - 43s 18ms/step - loss: 1.0584 - acc: 0.8640 - val_loss: 15.9835 - val_acc: 0.0071\n",
      "Epoch 27/150\n",
      "2405/2405 [==============================] - 44s 18ms/step - loss: 1.0555 - acc: 0.8640 - val_loss: 16.0739 - val_acc: 0.0071\n",
      "Epoch 28/150\n",
      "2405/2405 [==============================] - 42s 17ms/step - loss: 1.0544 - acc: 0.8636 - val_loss: 16.0900 - val_acc: 0.0071\n",
      "Epoch 29/150\n",
      "2405/2405 [==============================] - 41s 17ms/step - loss: 1.0401 - acc: 0.8649 - val_loss: 16.0888 - val_acc: 0.0071\n",
      "Epoch 30/150\n",
      "2405/2405 [==============================] - 40s 17ms/step - loss: 1.0471 - acc: 0.8632 - val_loss: 16.0876 - val_acc: 0.0071\n",
      "Epoch 31/150\n",
      "2405/2405 [==============================] - 39s 16ms/step - loss: 1.0996 - acc: 0.8495 - val_loss: 15.9735 - val_acc: 0.0142\n",
      "Epoch 32/150\n",
      "2405/2405 [==============================] - 39s 16ms/step - loss: 1.0474 - acc: 0.8632 - val_loss: 15.9726 - val_acc: 0.0142\n",
      "Epoch 33/150\n",
      "2405/2405 [==============================] - 43s 18ms/step - loss: 1.0387 - acc: 0.8632 - val_loss: 16.0870 - val_acc: 0.0071\n",
      "Epoch 34/150\n",
      "2405/2405 [==============================] - 45s 19ms/step - loss: 1.0354 - acc: 0.8624 - val_loss: 15.8578 - val_acc: 0.0213\n",
      "Epoch 35/150\n",
      "2405/2405 [==============================] - 43s 18ms/step - loss: 1.0132 - acc: 0.8665 - val_loss: 15.9709 - val_acc: 0.0142\n",
      "Epoch 36/150\n",
      "2405/2405 [==============================] - 50s 21ms/step - loss: 1.1237 - acc: 0.8395 - val_loss: 16.0886 - val_acc: 0.0071\n",
      "Epoch 37/150\n",
      "2405/2405 [==============================] - 45s 19ms/step - loss: 1.0844 - acc: 0.8528 - val_loss: 16.0895 - val_acc: 0.0071\n",
      "Epoch 38/150\n",
      "2405/2405 [==============================] - 44s 18ms/step - loss: 1.2652 - acc: 0.8042 - val_loss: 15.8712 - val_acc: 0.0213\n",
      "Epoch 39/150\n",
      "2405/2405 [==============================] - 41s 17ms/step - loss: 1.0866 - acc: 0.8549 - val_loss: 15.9848 - val_acc: 0.0142\n",
      "Epoch 40/150\n",
      "2405/2405 [==============================] - 42s 18ms/step - loss: 1.0680 - acc: 0.8561 - val_loss: 15.8703 - val_acc: 0.0213\n",
      "Epoch 41/150\n",
      "2405/2405 [==============================] - 40s 17ms/step - loss: 1.0415 - acc: 0.8640 - val_loss: 15.9836 - val_acc: 0.0142\n",
      "Epoch 42/150\n",
      "2405/2405 [==============================] - 39s 16ms/step - loss: 1.0386 - acc: 0.8653 - val_loss: 15.8895 - val_acc: 0.0142\n",
      "Epoch 43/150\n",
      "2405/2405 [==============================] - 38s 16ms/step - loss: 1.0335 - acc: 0.8661 - val_loss: 15.9812 - val_acc: 0.0142\n",
      "Epoch 44/150\n",
      "2405/2405 [==============================] - 39s 16ms/step - loss: 1.0337 - acc: 0.8657 - val_loss: 15.9799 - val_acc: 0.0142\n",
      "Epoch 45/150\n",
      "2405/2405 [==============================] - 38s 16ms/step - loss: 1.0328 - acc: 0.8661 - val_loss: 15.9788 - val_acc: 0.0142\n",
      "Epoch 46/150\n",
      "2405/2405 [==============================] - 39s 16ms/step - loss: 1.0303 - acc: 0.8653 - val_loss: 15.9776 - val_acc: 0.0142\n",
      "Epoch 47/150\n",
      "2405/2405 [==============================] - 39s 16ms/step - loss: 1.0283 - acc: 0.8661 - val_loss: 15.9766 - val_acc: 0.0142\n",
      "Epoch 48/150\n",
      "2405/2405 [==============================] - 38s 16ms/step - loss: 1.0273 - acc: 0.8657 - val_loss: 15.9755 - val_acc: 0.0142\n",
      "Epoch 49/150\n",
      "2405/2405 [==============================] - 39s 16ms/step - loss: 1.0255 - acc: 0.8661 - val_loss: 15.9745 - val_acc: 0.0142\n",
      "Epoch 50/150\n",
      "2405/2405 [==============================] - 39s 16ms/step - loss: 1.0242 - acc: 0.8657 - val_loss: 15.9735 - val_acc: 0.0142\n",
      "Epoch 51/150\n",
      "2405/2405 [==============================] - 39s 16ms/step - loss: 1.0232 - acc: 0.8653 - val_loss: 15.9725 - val_acc: 0.0142\n",
      "Epoch 52/150\n",
      "2405/2405 [==============================] - 38s 16ms/step - loss: 1.0153 - acc: 0.8661 - val_loss: 15.9716 - val_acc: 0.0142\n",
      "Epoch 53/150\n",
      "2405/2405 [==============================] - 38s 16ms/step - loss: 1.0148 - acc: 0.8657 - val_loss: 15.8662 - val_acc: 0.0142\n",
      "Epoch 54/150\n",
      "2405/2405 [==============================] - 38s 16ms/step - loss: 1.0213 - acc: 0.8661 - val_loss: 15.8588 - val_acc: 0.0213\n",
      "Epoch 55/150\n",
      "2405/2405 [==============================] - 38s 16ms/step - loss: 1.0195 - acc: 0.8661 - val_loss: 15.8548 - val_acc: 0.0213\n",
      "Epoch 56/150\n",
      "2405/2405 [==============================] - 38s 16ms/step - loss: 1.3193 - acc: 0.7938 - val_loss: 15.4084 - val_acc: 0.0426\n",
      "Epoch 57/150\n",
      "2405/2405 [==============================] - 38s 16ms/step - loss: 1.0796 - acc: 0.8507 - val_loss: 15.4796 - val_acc: 0.0426\n",
      "Epoch 58/150\n",
      "2405/2405 [==============================] - 39s 16ms/step - loss: 1.0363 - acc: 0.8644 - val_loss: 15.6289 - val_acc: 0.0355\n",
      "Epoch 59/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2405/2405 [==============================] - 38s 16ms/step - loss: 1.0231 - acc: 0.8657 - val_loss: 15.7886 - val_acc: 0.0213\n",
      "Epoch 60/150\n",
      "2405/2405 [==============================] - 39s 16ms/step - loss: 1.0209 - acc: 0.8653 - val_loss: 15.7738 - val_acc: 0.0213\n",
      "Epoch 61/150\n",
      "2405/2405 [==============================] - 38s 16ms/step - loss: 1.0205 - acc: 0.8653 - val_loss: 15.8550 - val_acc: 0.0213\n",
      "Epoch 62/150\n",
      "2405/2405 [==============================] - 38s 16ms/step - loss: 1.0192 - acc: 0.8661 - val_loss: 15.8542 - val_acc: 0.0213\n",
      "Epoch 63/150\n",
      "2405/2405 [==============================] - 38s 16ms/step - loss: 1.0182 - acc: 0.8661 - val_loss: 15.8533 - val_acc: 0.0213\n",
      "Epoch 64/150\n",
      "2405/2405 [==============================] - 38s 16ms/step - loss: 1.0169 - acc: 0.8657 - val_loss: 15.8525 - val_acc: 0.0213\n",
      "Epoch 65/150\n",
      "2405/2405 [==============================] - 38s 16ms/step - loss: 1.0158 - acc: 0.8657 - val_loss: 15.8517 - val_acc: 0.0213\n",
      "Epoch 66/150\n",
      "2405/2405 [==============================] - 38s 16ms/step - loss: 1.0153 - acc: 0.8661 - val_loss: 15.8509 - val_acc: 0.0213\n",
      "Epoch 67/150\n",
      "2405/2405 [==============================] - 38s 16ms/step - loss: 1.0142 - acc: 0.8661 - val_loss: 15.8500 - val_acc: 0.0213\n",
      "Epoch 68/150\n",
      "2405/2405 [==============================] - 37s 16ms/step - loss: 1.0134 - acc: 0.8661 - val_loss: 15.8492 - val_acc: 0.0213\n",
      "Epoch 69/150\n",
      "2405/2405 [==============================] - 38s 16ms/step - loss: 1.0125 - acc: 0.8657 - val_loss: 15.8484 - val_acc: 0.0213\n",
      "Epoch 70/150\n",
      "2405/2405 [==============================] - 38s 16ms/step - loss: 1.0120 - acc: 0.8661 - val_loss: 15.8475 - val_acc: 0.0213\n",
      "Epoch 71/150\n",
      "2405/2405 [==============================] - 38s 16ms/step - loss: 1.0110 - acc: 0.8653 - val_loss: 15.8467 - val_acc: 0.0213\n",
      "Epoch 72/150\n",
      "2405/2405 [==============================] - 38s 16ms/step - loss: 1.0102 - acc: 0.8661 - val_loss: 15.8458 - val_acc: 0.0213\n",
      "Epoch 73/150\n",
      "2405/2405 [==============================] - 38s 16ms/step - loss: 1.0089 - acc: 0.8657 - val_loss: 15.8450 - val_acc: 0.0213\n",
      "Epoch 74/150\n",
      "2405/2405 [==============================] - 38s 16ms/step - loss: 1.0084 - acc: 0.8661 - val_loss: 15.8441 - val_acc: 0.0213\n",
      "Epoch 75/150\n",
      "2405/2405 [==============================] - 38s 16ms/step - loss: 1.0076 - acc: 0.8657 - val_loss: 15.8434 - val_acc: 0.0213\n",
      "Epoch 76/150\n",
      "2405/2405 [==============================] - 38s 16ms/step - loss: 1.0137 - acc: 0.8649 - val_loss: 15.8288 - val_acc: 0.0213\n",
      "Epoch 77/150\n",
      "2405/2405 [==============================] - 38s 16ms/step - loss: 1.0291 - acc: 0.8607 - val_loss: 15.7278 - val_acc: 0.0284\n",
      "Epoch 78/150\n",
      "2405/2405 [==============================] - 38s 16ms/step - loss: 1.0333 - acc: 0.8620 - val_loss: 15.7275 - val_acc: 0.0284\n",
      "Epoch 79/150\n",
      "2405/2405 [==============================] - 38s 16ms/step - loss: 1.0251 - acc: 0.8636 - val_loss: 15.3840 - val_acc: 0.0496\n",
      "Epoch 80/150\n",
      "2405/2405 [==============================] - 38s 16ms/step - loss: 1.0362 - acc: 0.8578 - val_loss: 15.6125 - val_acc: 0.0355\n",
      "Epoch 81/150\n",
      "2405/2405 [==============================] - 38s 16ms/step - loss: 1.0983 - acc: 0.8407 - val_loss: 15.3860 - val_acc: 0.0496\n",
      "Epoch 82/150\n",
      "2405/2405 [==============================] - 38s 16ms/step - loss: 1.0915 - acc: 0.8453 - val_loss: 15.0021 - val_acc: 0.0709\n",
      "Epoch 83/150\n",
      "2405/2405 [==============================] - 39s 16ms/step - loss: 1.0639 - acc: 0.8528 - val_loss: 15.0908 - val_acc: 0.0638\n",
      "Epoch 84/150\n",
      "2405/2405 [==============================] - 38s 16ms/step - loss: 1.0429 - acc: 0.8553 - val_loss: 15.5000 - val_acc: 0.0426\n",
      "Epoch 85/150\n",
      "2405/2405 [==============================] - 38s 16ms/step - loss: 1.0262 - acc: 0.8632 - val_loss: 15.2706 - val_acc: 0.0567\n",
      "Epoch 86/150\n",
      "2405/2405 [==============================] - 38s 16ms/step - loss: 1.1094 - acc: 0.8453 - val_loss: 15.6147 - val_acc: 0.0355\n",
      "Epoch 87/150\n",
      "2405/2405 [==============================] - 39s 16ms/step - loss: 1.2601 - acc: 0.8029 - val_loss: 16.1902 - val_acc: 0.0000e+00\n",
      "Epoch 88/150\n",
      "2405/2405 [==============================] - 38s 16ms/step - loss: 1.0765 - acc: 0.8582 - val_loss: 15.9609 - val_acc: 0.0142\n",
      "Epoch 89/150\n",
      "2405/2405 [==============================] - 38s 16ms/step - loss: 1.0532 - acc: 0.8620 - val_loss: 15.9520 - val_acc: 0.0142\n",
      "Epoch 90/150\n",
      "2405/2405 [==============================] - 38s 16ms/step - loss: 1.0429 - acc: 0.8640 - val_loss: 16.1880 - val_acc: 0.0000e+00\n",
      "Epoch 91/150\n",
      "2405/2405 [==============================] - 38s 16ms/step - loss: 1.0476 - acc: 0.8636 - val_loss: 16.1871 - val_acc: 0.0000e+00\n",
      "Epoch 92/150\n",
      "2405/2405 [==============================] - 38s 16ms/step - loss: 1.0399 - acc: 0.8640 - val_loss: 16.1862 - val_acc: 0.0000e+00\n",
      "Epoch 93/150\n",
      "2405/2405 [==============================] - 38s 16ms/step - loss: 1.0457 - acc: 0.8636 - val_loss: 16.1853 - val_acc: 0.0000e+00\n",
      "Epoch 94/150\n",
      "2405/2405 [==============================] - 38s 16ms/step - loss: 1.0446 - acc: 0.8632 - val_loss: 16.1844 - val_acc: 0.0000e+00\n",
      "Epoch 95/150\n",
      "2405/2405 [==============================] - 39s 16ms/step - loss: 1.0371 - acc: 0.8636 - val_loss: 16.1835 - val_acc: 0.0000e+00\n",
      "Epoch 96/150\n",
      "2405/2405 [==============================] - 38s 16ms/step - loss: 1.0423 - acc: 0.8636 - val_loss: 16.1826 - val_acc: 0.0000e+00\n",
      "Epoch 97/150\n",
      "2405/2405 [==============================] - 39s 16ms/step - loss: 1.0418 - acc: 0.8628 - val_loss: 16.1817 - val_acc: 0.0000e+00\n",
      "Epoch 98/150\n",
      "2405/2405 [==============================] - 38s 16ms/step - loss: 1.0407 - acc: 0.8636 - val_loss: 16.1809 - val_acc: 0.0000e+00\n",
      "Epoch 99/150\n",
      "2405/2405 [==============================] - 38s 16ms/step - loss: 1.0399 - acc: 0.8632 - val_loss: 16.1800 - val_acc: 0.0000e+00\n",
      "Epoch 100/150\n",
      "2405/2405 [==============================] - 38s 16ms/step - loss: 1.0388 - acc: 0.8632 - val_loss: 16.1791 - val_acc: 0.0000e+00\n",
      "Epoch 101/150\n",
      "2405/2405 [==============================] - 38s 16ms/step - loss: 1.0381 - acc: 0.8636 - val_loss: 16.1783 - val_acc: 0.0000e+00\n",
      "Epoch 102/150\n",
      "2405/2405 [==============================] - 38s 16ms/step - loss: 1.0374 - acc: 0.8636 - val_loss: 16.1774 - val_acc: 0.0000e+00\n",
      "Epoch 103/150\n",
      "2405/2405 [==============================] - 39s 16ms/step - loss: 1.0365 - acc: 0.8628 - val_loss: 16.1765 - val_acc: 0.0000e+00\n",
      "Epoch 104/150\n",
      "2405/2405 [==============================] - 38s 16ms/step - loss: 1.0352 - acc: 0.8636 - val_loss: 16.1757 - val_acc: 0.0000e+00\n",
      "Epoch 105/150\n",
      "2405/2405 [==============================] - 38s 16ms/step - loss: 1.0344 - acc: 0.8636 - val_loss: 16.1748 - val_acc: 0.0000e+00\n",
      "Epoch 106/150\n",
      "2405/2405 [==============================] - 38s 16ms/step - loss: 1.0331 - acc: 0.8628 - val_loss: 16.1740 - val_acc: 0.0000e+00\n",
      "Epoch 107/150\n",
      "2405/2405 [==============================] - 38s 16ms/step - loss: 1.0330 - acc: 0.8636 - val_loss: 16.1731 - val_acc: 0.0000e+00\n",
      "Epoch 108/150\n",
      "2405/2405 [==============================] - 38s 16ms/step - loss: 1.0326 - acc: 0.8628 - val_loss: 16.1722 - val_acc: 0.0000e+00\n",
      "Epoch 109/150\n",
      "2405/2405 [==============================] - 38s 16ms/step - loss: 1.0313 - acc: 0.8636 - val_loss: 16.1714 - val_acc: 0.0000e+00\n",
      "Epoch 110/150\n",
      "2405/2405 [==============================] - 38s 16ms/step - loss: 1.0302 - acc: 0.8628 - val_loss: 16.1705 - val_acc: 0.0000e+00\n",
      "Epoch 111/150\n",
      "2405/2405 [==============================] - 38s 16ms/step - loss: 1.0297 - acc: 0.8632 - val_loss: 16.1697 - val_acc: 0.0000e+00\n",
      "Epoch 112/150\n",
      "2405/2405 [==============================] - 38s 16ms/step - loss: 1.0286 - acc: 0.8636 - val_loss: 16.1688 - val_acc: 0.0000e+00\n",
      "Epoch 113/150\n",
      "2405/2405 [==============================] - 39s 16ms/step - loss: 1.0272 - acc: 0.8636 - val_loss: 16.1680 - val_acc: 0.0000e+00\n",
      "Epoch 114/150\n",
      "2405/2405 [==============================] - 38s 16ms/step - loss: 1.0276 - acc: 0.8636 - val_loss: 16.1671 - val_acc: 0.0000e+00\n",
      "Epoch 115/150\n",
      "2405/2405 [==============================] - 38s 16ms/step - loss: 1.0264 - acc: 0.8636 - val_loss: 16.1663 - val_acc: 0.0000e+00\n",
      "Epoch 116/150\n",
      "2405/2405 [==============================] - 39s 16ms/step - loss: 1.0251 - acc: 0.8632 - val_loss: 16.1654 - val_acc: 0.0000e+00\n",
      "Epoch 117/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2405/2405 [==============================] - 38s 16ms/step - loss: 1.0241 - acc: 0.8636 - val_loss: 16.1646 - val_acc: 0.0000e+00\n",
      "Epoch 118/150\n",
      "2405/2405 [==============================] - 38s 16ms/step - loss: 1.0241 - acc: 0.8632 - val_loss: 16.1637 - val_acc: 0.0000e+00\n",
      "Epoch 119/150\n",
      "2405/2405 [==============================] - 38s 16ms/step - loss: 1.0226 - acc: 0.8636 - val_loss: 16.1629 - val_acc: 0.0000e+00\n",
      "Epoch 120/150\n",
      "2405/2405 [==============================] - 38s 16ms/step - loss: 1.0218 - acc: 0.8636 - val_loss: 16.1621 - val_acc: 0.0000e+00\n",
      "Epoch 121/150\n",
      "2405/2405 [==============================] - 38s 16ms/step - loss: 1.0214 - acc: 0.8636 - val_loss: 16.1612 - val_acc: 0.0000e+00\n",
      "Epoch 122/150\n",
      "2405/2405 [==============================] - 38s 16ms/step - loss: 1.0212 - acc: 0.8632 - val_loss: 16.1604 - val_acc: 0.0000e+00\n",
      "Epoch 123/150\n",
      "2405/2405 [==============================] - 38s 16ms/step - loss: 1.0409 - acc: 0.8595 - val_loss: 16.1611 - val_acc: 0.0000e+00\n",
      "Epoch 124/150\n",
      "2405/2405 [==============================] - 39s 16ms/step - loss: 1.1039 - acc: 0.8420 - val_loss: 15.2511 - val_acc: 0.0567\n",
      "Epoch 125/150\n",
      "2405/2405 [==============================] - 44s 18ms/step - loss: 1.1006 - acc: 0.8374 - val_loss: 15.6072 - val_acc: 0.0284\n",
      "Epoch 126/150\n",
      "2405/2405 [==============================] - 42s 18ms/step - loss: 1.0495 - acc: 0.8499 - val_loss: 15.4832 - val_acc: 0.0426\n",
      "Epoch 127/150\n",
      "2405/2405 [==============================] - 40s 17ms/step - loss: 1.0347 - acc: 0.8574 - val_loss: 14.9112 - val_acc: 0.0780\n",
      "Epoch 128/150\n",
      "2405/2405 [==============================] - 40s 17ms/step - loss: 1.0227 - acc: 0.8590 - val_loss: 15.9400 - val_acc: 0.0142\n",
      "Epoch 129/150\n",
      "2405/2405 [==============================] - 44s 18ms/step - loss: 1.0056 - acc: 0.8644 - val_loss: 15.1385 - val_acc: 0.0638\n",
      "Epoch 130/150\n",
      "2405/2405 [==============================] - 47s 19ms/step - loss: 0.9950 - acc: 0.8653 - val_loss: 15.8231 - val_acc: 0.0213\n",
      "Epoch 131/150\n",
      "2405/2405 [==============================] - 42s 17ms/step - loss: 1.0051 - acc: 0.8644 - val_loss: 15.3404 - val_acc: 0.0426\n",
      "Epoch 132/150\n",
      "2405/2405 [==============================] - 47s 19ms/step - loss: 0.9886 - acc: 0.8653 - val_loss: 15.7060 - val_acc: 0.0284\n",
      "Epoch 133/150\n",
      "2405/2405 [==============================] - 46s 19ms/step - loss: 1.0154 - acc: 0.8620 - val_loss: 15.8197 - val_acc: 0.0213\n",
      "Epoch 134/150\n",
      "2405/2405 [==============================] - 33s 14ms/step - loss: 1.0149 - acc: 0.8636 - val_loss: 15.5329 - val_acc: 0.0355\n",
      "Epoch 135/150\n",
      "2405/2405 [==============================] - 30s 13ms/step - loss: 1.0128 - acc: 0.8640 - val_loss: 15.5885 - val_acc: 0.0355\n",
      "Epoch 136/150\n",
      "2405/2405 [==============================] - 32s 13ms/step - loss: 1.0109 - acc: 0.8640 - val_loss: 15.7015 - val_acc: 0.0284\n",
      "Epoch 137/150\n",
      "2405/2405 [==============================] - 32s 13ms/step - loss: 1.0097 - acc: 0.8640 - val_loss: 15.8145 - val_acc: 0.0213\n",
      "Epoch 138/150\n",
      "2405/2405 [==============================] - 31s 13ms/step - loss: 1.0085 - acc: 0.8636 - val_loss: 15.8133 - val_acc: 0.0213\n",
      "Epoch 139/150\n",
      "2405/2405 [==============================] - 33s 14ms/step - loss: 1.0075 - acc: 0.8636 - val_loss: 15.8121 - val_acc: 0.0213\n",
      "Epoch 140/150\n",
      "2405/2405 [==============================] - 31s 13ms/step - loss: 1.0064 - acc: 0.8636 - val_loss: 15.8110 - val_acc: 0.0213\n",
      "Epoch 141/150\n",
      "2405/2405 [==============================] - 30s 12ms/step - loss: 1.0050 - acc: 0.8632 - val_loss: 15.8099 - val_acc: 0.0213\n",
      "Epoch 142/150\n",
      "2405/2405 [==============================] - 31s 13ms/step - loss: 1.0046 - acc: 0.8640 - val_loss: 15.8089 - val_acc: 0.0213\n",
      "Epoch 143/150\n",
      "2405/2405 [==============================] - 32s 13ms/step - loss: 1.0031 - acc: 0.8640 - val_loss: 15.8079 - val_acc: 0.0213\n",
      "Epoch 144/150\n",
      "2405/2405 [==============================] - 32s 13ms/step - loss: 1.0023 - acc: 0.8640 - val_loss: 15.9213 - val_acc: 0.0142\n",
      "Epoch 145/150\n",
      "2405/2405 [==============================] - 32s 13ms/step - loss: 1.0088 - acc: 0.8636 - val_loss: 15.9204 - val_acc: 0.0142\n",
      "Epoch 146/150\n",
      "2405/2405 [==============================] - 32s 13ms/step - loss: 1.0006 - acc: 0.8640 - val_loss: 15.6909 - val_acc: 0.0284\n",
      "Epoch 147/150\n",
      "2405/2405 [==============================] - 32s 13ms/step - loss: 1.0067 - acc: 0.8636 - val_loss: 15.9187 - val_acc: 0.0142\n",
      "Epoch 148/150\n",
      "2405/2405 [==============================] - 30s 13ms/step - loss: 1.0058 - acc: 0.8636 - val_loss: 15.9179 - val_acc: 0.0142\n",
      "Epoch 149/150\n",
      "2405/2405 [==============================] - 32s 13ms/step - loss: 1.0120 - acc: 0.8632 - val_loss: 15.9171 - val_acc: 0.0142\n",
      "Epoch 150/150\n",
      "2405/2405 [==============================] - 31s 13ms/step - loss: 0.9981 - acc: 0.8640 - val_loss: 15.6878 - val_acc: 0.0284\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1c1e4cd0b8>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_tensor, y_labels, validation_data=(X_test_all, y_test_enc), epochs=150)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MAX 12.06%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
